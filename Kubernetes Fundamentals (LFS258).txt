*** Kubernetes ***
an open-source software for automating deployment, scaling, and management of containerized applications.
Instead of using a large server, Kubernetes approaches the same issue by deploying a large number of small servers, or microservices.

Communication is entirely API call-driven, which allows for flexibility. 
Cluster configuration information is stored in a JSON format inside of etcd, but is most often written in YAML by the community. 
Kubernetes agents convert the YAML to JSON prior to persistence to the database.

Containers provide a great way to package, ship, and run applications - that is the Docker motto. 

Continuous integration/continuous delivery (CI/CD) pipeline to build, test and verify container images.
Spinnaker, Jenkins and Helm can be helpful to use, among other possible tools.

Managing Containerized Applications
Docker Swarm
Apache Mesos
Nomad
Rancher

Kubernetes is inspired by Borg - the internal system used by Google to manage its applications (e.g. Gmail, Apps, GCE).

>>Kubernetes Architecture
Kubernetes is made of control plane nodes (aka cp nodes) and worker nodes, once called minions.
The cp runs an API server, a scheduler, various controllers and a storage system to keep the state of the cluster, container settings, and the networking configuration.
Every node running a container would have kubelet and kube-proxy.

kubectl:  You can communicate with the API using a local client called kubectl
curl:  you can write your own client and use curl
kube-scheduler:  is forwarded the pod spec for running containers coming to the API and finds a suitable node to run those containers
kubelet: receives requests to run the containers, manages any resources necessary and works with the container engine to manage them on the local node
kube-proxy: creates and manages networking rules to expose the container on the network to other containers or the outside world.

Pod: consists of one or more containers which share an IP address, access to storage and namespace. 
Namespaces: Kubernetes uses namespaces to keep objects distinct from each other, for resource control and multi-tenant considerations.
Operators: Orchestration is managed through a series of watch-loops, also called controllers
Deployment: The default and feature-filled operator for containers is a Deployment.

kube-apiserver: Each controller interrogates the kube-apiserver for a particular object state
kube-controller-manager: controllers are compiled into the kube-controller-manager, but others can be added using custom resource definitions.


*** INSTALLATION AND CONFIGURATION ***
Kubernetes cluster, you will need to have installed the Kubernetes command line, called kubectl, or a wrapper command such as gcloud
gcloud = kubectl

kubeadm: the community-suggested tool from the Kubernetes project, that makes installing Kubernetes easy and avoids vendor-specific installers.
kubeadm init: you run on a cp node
kubeadm join: you run on your worker or redundant cp nodes, and your cluster bootstraps itself

>>create your first Kubernetes cluster:
$ gcloud container clusters create linuxfoundation \
--region=europe-central2-a
$ gcloud container clusters list
$ kubectl get nodes
$ gcloud container clusters delete linuxfoundation

>> Using Minikube
$ curl -Lo minikube ht‌tps://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64
$ chmod +x minikube
$ sudo mv minikube /usr/local/bin
$ minikube start
$ kubectl get nodes

>> Installing with kubeadm
Once the cp has initialized, you would apply a network plugin. Main steps:
Run kubeadm init on the control plane node.
Create a network for IP-per-Pod criteria.
Run kubeadm join on workers or secondary cp nodes.
$ kubectl create -f ht‌tps://git.io/weave-kube		example

kubeadm-upgrade: option to upgrade the cluster using the kubeadm upgrade command.
-plan -apply - diff -node

>> Installing a Pod Network
Prior to initializing the Kubernetes cluster, the network must be considered and IP conflicts avoided.
Calico
Flannel
Kube-router
Cilium

>>More Installation Tools
Kubespray
kops
kube-aws
kind

>> Main Deployment Configurations
Single-node
With a single-node deployment, all the components run on the same server.
Single head node, multiple workers
Adding more workers, a single head node and multiple workers typically will consist of a single node etcd instance running on the head node with the API, the scheduler, and the controller-manager.
Multiple head nodes with HA, multiple workers
Multiple head nodes in an HA configuration and multiple workers add more durability to the cluster. 
HA etcd, HA head nodes, multiple workers
The most advanced and resilient setup would be an HA etcd cluster, with HA head nodes and multiple workers.


*** KUBERNETES ARCHITECTURE ***
>>Kubernetes has the following main components:
Control plane(s) and worker node(s)
Operators
Services
Pods of containers
Namespaces and quotas
Network and policies
Storage.

>> Control Plane Node
kube-apiserver: is central to the operation of the Kubernetes cluster. All calls, both internal and external traffic, are handled via this agent. 
kube-scheduler: uses an algorithm to determine which node will host a Pod of containers. 
kube-controller-manager: is a core control loop daemon which interacts with the kube-apiserver to determine the state of the cluster.
kube-dns:  To handle DNS queries

>>Kubelet
kubelet systemd process is the heavy lifter for changes and configuration on worker nodes.
It accepts the API calls for Pod specifications
It will work to configure the local node until the specification has been met.

Uses PodSpec
Mounts volumes to Pod
Downloads secrets
Passes request to local container engine
Reports status of Pods and node to cluster(kube-apiserver).

>> Operators
operators, otherwise known as controllers or watch-loops

Using a DeltaFIFO queue, the source and downstream are compared. 
A loop process receives an obj or object, which is an array of deltas from the FIFO queue.

endpoints, namespace, and serviceaccounts operators each manage the eponymous resources for Pods.

>service operator
service operator sends messages via the kube-apiserver which forwards settings to kube-proxy on every node, as well as the network plugin such as calico-kube-controllers.
Connect Pods together
Expose Pods to Internet
Decouple settings
Define Pod access policy.

>> Pods
the smallest unit we can work with is a Pod. We do not interact with particular containers.
InitContainers can order startup, to some extent.

There is only one IP address per Pod
If there is more than one container in a pod, they must share the IP. 

sidecar for a container dedicated to performing a helper task, like handling logs and responding to requests, as the primary application container may not have this ability. 

>> Containers
we can manage the resources containers are allowed to consume. 

PodSpec you can pass parameters which will be passed to the container runtime on the scheduled node: 
resources:
  limits: 
    cpu: "1"
    memory: "4Gi" 
  requests:
    cpu: "0.5"
    memory: "500Mi"

ResourceQuota: we can manage the resources containers are allowed to consume. 

*LivenessProbes, ReadinessProbes, and StatefulSets can be used to determine the order

>> Init Containers
Init container, must complete before app containers will be started.
Init containers can contain code or utilities that are not in an app.
init container until the ls command succeeds; then the database container will start.

spec:
  containers:
  - name: main-app
    image: databaseD 
  initContainers:
  - name: wait-database
    image: busybox
    command: ['sh', '-c', 'until ls /db/dir ; do sleep 5; done; ']

>> Kubernetes Architectural Review

etcdctl command to interrogate the database
calicoctl to view more of how the network is configured

> Felix, which is the primary Calico agent on each machine. 
This agent, or daemon, is responsible for interface monitoring and management, route programming, ACL configuration and state reporting.
BIRD is a dynamic IP routing daemon used by Felix to read routing state and distribute that information to other nodes in the cluster.

-kubectl (api call)
--kube-apiserver
---kube-controller-managaer
----etcd (asks to database)
-----kube-scheduler (return to kube-api)
------kubelet (connect to container kubelet)
-------kube-proxy (connects containers
--------kubelet in container (create container engine)

>>Node
 node is an API object created outside the cluster representing an instance.
Once the node has the necessary software installed, it is ingested into the API server.

you can create a cp node with the "kubeadm init" command and worker nodes by passing "join".
NodeLease will schedule the node for deletion (kube-apiserver cannot communicate with the kubelet on a node for 5 minutes)
NodeStatus will change from ready.
kube-node-lease
kubectl delete node <node-name> 	to remove it from the API server.

>> Pod Network
inter-process communication (IPC): To communicate with each other, containers within pods can use the loopback interface, write to files on a common filesystem
All containers in a pod share the same network namespace.

>> Container/Services Networking
NodePort service connects the Pod to the outside network. 

>> Services
connect one pod to another, or to outside of the cluster.

>Service Network
ClusterIP which is used to connect inside the cluster, not the IP of the cluster
pause container: cluster to reserve the IP address in the namespace prior to starting the other pods

>>CNI Network Configuration File
CNI is an emerging specification with associated libraries to write plugins that configure container networking and remove allocated resources when the container is deleted.
goal of kubeadm (the Kubernetes cluster bootstrapping tool) has been to use CNI, but you may need to recompile to do so.

With CNI, you can write a network configuration file:
{
    "cniVersion": "0.2.0",
    "name": "mynet",
    "type": "bridge",
    "bridge": "cni0",		//standart linux bridge
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "subnet": "10.22.0.0/16",
        "routes": [
            { "dst": "0.0.0.0/0" }
             ]
     }
}

>Pod-to-Pod Communication
The requirement from Kubernetes is the following:

All pods can communicate with each other across nodes.
All nodes can communicate with all pods.
No Network Address Translation (NAT).


*** APIS AND ACCESS ***

>> RESTful
kubectl makes API calls on your behalf, responding to typical HTTP verbs (GET, POST, DELETE).
curl: make calls externally

>> Checking Access
The following shows what user bob could do in the default namespace and the developer namespace, using the auth can-i subcommand to query (commands and outputs): 
$ kubectl auth can-i create deployments
$ kubectl auth can-i create deployments --as bob
$ kubectl auth can-i create deployments --as bob --namespace developer

>SelfSubjectAccessReview​
	Access review for any user, helpful for delegating to others. 
>LocalSubjectAccessReview
​	Review is restricted to a specific namespace.
>SelfSubjectRulesReview​
	A review which shows allowed actions for a user within a particular namespace. 

>> Annotations
annotations allow for metadata to be included with an object that may be helpful outside of the Kubernetes object interaction.
The annotation data could otherwise be held in an exterior databas
can be used to track information such as a timestamp, pointers to related objects from other ecosystems,

For example, to annotate only Pods within a namespace, you can overwrite the annotation, and finally delete it. See the following commands:
$ kubectl annotate pods --all description='Production Pods' -n prod 
$ kubectl annotate --overwrite pod webpod description="Old Production Pods" -n prod 
$ kubectl -n prod annotate pod webpod description-

>> Simple Pod
ApiVersion (it must match the existing API group),
kind (the type of object to create), 
metadata (at least a name)
spec (what to create and parameters)

apiVersion: v1
kind: Pod
metadata:
    name: firstpod
spec:
    containers:
    - image: nginx
      name: stan 

kubectl create command to create this pod in Kubernetes
kubectl get pods. check its status

$ kubectl create -f simple.yaml 
$ kubectl get pods 
$ kubectl get pod firstpod -o yaml 
$ kubectl get pod firstpod -o json

>> Manage API Resources with kubectl
The state of the resources can be changed using standard HTTP verbs (e.g. GET, POST, PATCH, DELETE
details from where the command gets and updates information

$ kubectl --v=10 get pods firstpod
$ kubectl --v=10 delete pods firstpod

>> Access from Outside the Cluster
The primary tool used from the command line will be "kubectl", which calls "curl" on your behalf.

curl command from outside the cluster to view or make changes. 

$ kubectl config view 

>> ~/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdF.....
    server: ht‌tps://10.128.0.3:6443 ;
    name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTib.....
    client-key-data: LS0tLS1CRUdJTi....

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdF.....
    server: ht‌tps://10.128.0.3:6443 ;
    name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTib.....
    client-key-data: LS0tLS1CRUdJTi....


>> NameSpaces
keep resources distinct. 
Every API call includes a namespace
ht‌tps://10.128.0.3:6443/api/v1/namespaces/default/pods

kube-node-lease: This is the namespace where worker node lease information is kept.
kube-public: A namespace readable by all, even those not authenticated.
kube-system: This namespace contains infrastructure pods.

>> Working with Namespaces
$ kubectl get ns
$ kubectl create ns linuxcon
$ kubectl describe ns linuxcon
$ kubectl get ns/linuxcon -o yaml
$ kubectl delete ns/linuxco

Once a namespace has been created, you can reference it via YAML
$ cat redis.yaml

>> API Resources with kubectl
All API resources exposed are available via kubectl.
kubectl [command] [type] [Name] [flag]

*** API OBJECTS ***
resources in the v1 API group, among others. Stability increases and code becomes more stable as objects move from alpha versions, to beta, and then v1, indicating stability. 

>>v1 API Group
v1 API group is no longer a single group, but rather a collection of groups for each main object category. 
For example, there is a v1 group, a storage.k8s.io/v1 group, and an rbac.authorization.k8s.io/v1

You can turn on and off the scheduling to a node with the kubectl cordon/uncordon commands.
You can write a resourcequota manifest, create it with kubectl and the quota will be enforced.

>>Deploying an object
kubectl create
deployment, replicaset,pod

>>StatefulSets
StatefulSet is the workload API object used to manage stateful applications.
The default deployment scheme is sequential, starting with 0, such as app-0, app-1, app-2, etc. 
A following Pod will not launch until the current Pod reaches a running and ready state. 
They are not deployed in parallel.

>>Autoscaling
-Horizontal Pod Autoscalers (HPA). This is a stable resource. 
HPAs automatically scale Replication Controllers, ReplicaSets, or Deployments based on a target of 50% CPU usage by default.
-Cluster Autoscaler (CA) adds or removes nodes to the cluster, based on the inability to deploy a Pod or having nodes with low utilization for at least 10 minutes.
cluster-autoscaler-		nodes should be added and removed through

>>Jobs
Jobs are part of the batch API group. They are used to run a set number of pods to completion. 
If a pod fails, it will be restarted until the number of completion is reached.
.spec.concurrencyPolicy, which determines how to handle existing jobs
-Allow, the default, another concurrent job will be run
-Forbid, the current job continues and the new job is skipped.
-Replace, cancels the current job and starts a new job in its place.

>>RBAC
four resources: ClusterRole, Role, ClusterRoleBinding, and RoleBinding. 
They are used for Role Based Access Control (RBAC) to Kubernetes.
$ curl localhost:8080/apis/rbac.authorization.k8s.io/v1

*** MANAGING STATE WITH DEPLOYMENTS ***
kubectl run : default controller for a container deployed
kubectl apply : updates can be configured by editing a YAML file and running
kubectl edit : to modify the in-use configuration

>>Deployments
ReplicationControllers (RC) ensure that a specified number of pod replicas is running at any one time.,
a new resource was introduced in the apps/v1 API group: Deployments
Deployments generate ReplicaSets, which offer more selection features than ReplicationControllers, such as matchExpressions. 
$ kubectl create deployment dev-web --image=nginx:1.13.7-alpine

>>Object Relationship
1- controllers, or watch loops, running as a thread of the kube-controller-manager. 
2- Each controller queries the kube-apiserver for the current state of the object they track
3- The state of each object on a worker node is sent back from the local kubelet. 

>>Deployment Details
To generate the YAML file of the newly created objects, run the following command:
$ kubectl get deployments,rs,pods -o yaml
Sometimes, a JSON output can make it more clear. Try this command:
$ kubectl get deployments,rs,pods -o json

>>Deployment Configuration Metadata
metadata:			
  annotations:		provide further information that could be helpful to third-party applications or administrative tracking.
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: 2022-12-21T13:57:07Z		Shows when the object was originally created.
  generation: 1				How many times this object has been edited, such as changing the number of replicas, for example.	
  labels:				Arbitrary strings used to select or exclude objects for use with kubectl, or other API calls. 
    app: dev-web
  name: dev-web				This is a required string, which we passed from the command line. The name must be unique to the namespace.
  namespace: default
  resourceVersion: "774003"		A value tied to the etcd database to help with concurrency of objects. 
  uid: d52d3a63-e656-11e7-9319-42010a800003		Remains a unique ID for the life of the object.


>>Deployment Configuration Spec
There are two spec declarations for the deployment. 
The first will modify the ReplicaSet created, while the second will pass along the Pod configuration. 

spec:  					A declaration that the following items will configure the object being created.
  progressDeadlineSeconds: 600   	Time in seconds until a progress error is reported during a change. Reasons could be quotas, image issues, or limit ranges.
  replicas: 1  				this parameter determines how many Pods should be created.
  revisionHistoryLimit: 10   		How many old ReplicaSet specifications to retain for rollback.
  selector:     
    matchLabels:       			Set-based requirements of the Pod selector.
      app: dev-web  
  strategy:     			A header for values having to do with updating Pods. Works with the later listed type. Could also be set to Recreate, which would delete all existing pods before new pods are created.
    rollingUpdate:       		RollingUpdate, you can control how many Pods are deleted at a time with the following parameters.
      maxSurge: 25%        
      maxUnavailable: 25%     		A number or percentage of Pods which can be in a state other than Ready during the update process.
    type: RollingUpdate

>> Deployment Configuration Pod Template

template:					Data being passed to the ReplicaSet to determine how to deploy an object
  metadata:
    creationTimestamp: null
    labels:
      app: dev-web
  spec:
    containers:					Key word indicating that the following items of this indentation are for a container.
    - image: nginx:1.17.7-alpine		This is the image name passed to the container engine, typically Docker. 
      imagePullPolicy: IfNotPresent		Policy settings passed along to the container engine, about when and if an image should be downloaded or used from a local cache.
      name: dev-web				The leading stub of the Pod names. A unique string will be appended.
      resources: {}				set resource restrictions and settings, such as a limit on CPU or memory for the containers.
      terminationMessagePath: /dev/termination-log		A customizable location of where to output success or failure information of a container.
      terminationMessagePolicy: File
    dnsPolicy: ClusterFirst			Determines if DNS queries should go to coredns or, if set to Default, use the node's DNS resolution configuration.
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}				Flexible setting to pass one or more security settings, such as SELinux context
    terminationGracePeriodSeconds: 30		The amount of time to wait for a SIGTERM to run until a SIGKILL is used to terminate the container.

>availableReplicas
Indicates how many were configured by the ReplicaSet
>observedGeneration
observedGeneration

>>Scaling and Rolling Updates

$ kubectl scale deploy/dev-web --replicas=4
$ kubectl get deployments
$ kubectl edit deployment dev-web

>> Deployment Rollbacks

$ kubectl create deploy ghost --image=ghost
$ kubectl annotate deployment/ghost kubernetes.io/change-cause="kubectl create deploy ghost --image=ghost"
$ kubectl get deployments ghost -o yaml
$ kubectl set image deployment/ghost ghost=ghost:09 --all
$ kubectl get pods
// You can roll back to a specific revision with the --to-revision=2 option.
// You can also edit a Deployment using the kubectl edit command.
$ kubectl rollout pause deployment/ghost
​$ kubectl rollout resume deployment/ghost

>>Using DaemonSets
The use of a DaemonSet allows for ensuring a particular container is always running. 
In a large and dynamic environment, it can be helpful to have a logging or metric generation application on every node without an administrator remembering to deploy that application. 

Use kind: DaemonSet.​

>> Labels
Part of the metadata of an object is a label. 
Though labels are not API objects, they are an important tool for cluster administration.
You could then view labels in new columns (commands and outputs below): 

$ kubectl get pods -l run=ghost
$ kubectl get pods -L run
$ kubectl label pods ghost-3378155678-eq5i6 foo=bar
$ kubectl get pods --show-labels

if you want to force the scheduling of a pod on a specific node, you can use a nodeSelector in a pod definition, add specific labels to certain nodes in your cluster and use those labels in the pod. 


*** VOLUMES AND DATA ***
A particular access mode is part of a Pod request. 
As a request, the user may be granted more, but not less access, though a direct match is attempted first. 

ReadWriteOnce, which allows read-write by a single node
ReadOnlyMany, which allows read-only by multiple nodes
ReadWriteMany, which allows read-write by many nodes.

local kubelet uses the kubelet_pods.go script to map the raw devices
The API server makes a request for the storage to the StorageClass plugin, but the specifics of the requests to the backend storage depend on the plugin in use. 

 emptyDir. The kubelet will create the directory in the container, but not mount any storage.

>>Volume Types
In GCE or AWS, you can use volumes of type GCEpersistentDisk or awsElasticBlockStore, which allows you to mount GCE and EBS disks in your Pods,
The hostPath volume mounts a resource from the host node filesystem.

>>Shared Volume Example

containers:
   - name: alphacont
     image: busybox
     volumeMounts:
     - mountPath: /alphadir
       name: sharevol
   - name: betacont
     image: busybox
     volumeMounts:
     - mountPath: /betadir
       name: sharevol
   volumes:
   - name: sharevol
     emptyDir: {}   

>>Persistent Volumes and Claims
persistent volume (pv) is a storage abstraction used to retain data longer then the Pod using it.
persistentVolumeClaim (pvc) with various parameters for size
StorageClass: backend storage known as its 

The reclaim phase has three options:
1 Retain, which keeps the data intact, allowing for an administrator to handle the storage and data.
2 Delete tells the volume plugin to delete the API object, as well as the storage behind it. 
3 The Recycle option runs an rm -rf /mountpoint and then makes it available to a new claim.

$ kubectl get pv
$ kubectl get pvc

* Persistent volumes are not a namespaces object, but persistent volume claims are.

>> Persistent Volume Claim
Persistent volumes are not a namespaces object, but persistent volume claims are.
In the Pod, the volume uses the persistentVolumeClaim.

--In the Pod:

spec:
  containers:
....
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: myclaim


StorageClass API resource allows an administrator to define a persistent volume provisioner of a certain type, passing storage-specific parameters. 
-a user can request a claim, which the API Server fills via auto-provisioning.

StorageClass using GCE: 

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast      # Could be any name
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd 

>>Secrets

$ kubectl get secrets 
$ kubectl create secret generic --help
$ kubectl create secret generic mysql --from-literal=password=root

A secret is not encrypted, only base64-encoded, by default. 
You must create an EncryptionConfiguration with a key and proper identity. 
Then, the kube-apiserver needs the --encryption-provider-config flag set to a previously configured provider, such as aescbc or ksm. 
Once this is enabled, you need to recreate every secret, as they are encrypted upon write. 

secret can be made manually as well, then inserted into a YAML file
$ echo LFTr@1n | base64

TEZUckAxbgo=

$ vim secret.yaml

apiVersion: v1
kind: Secret
metadata:
  name: lf-secret
data:
  password: TEZUckAxbgo=

>>Using Secrets via Environment Variables
spec:
  containers:
  - image: mysql:5.5
    name: dbpod
    env:
      - name: MYSQL_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysql
            key: password 

there is a 1MB limit to their size. very large numbers of secrets could deplete memory on a host.

>>Mounting Secrets as Volumes

Once the pod is running, you can verify that the secret is indeed accessible in the container by running this command 
$ kubectl exec -ti busybox -- cat /mysqlpassword/password

>>Portable Data with ConfigMaps
ConfigMap decouples a container image from configuration artifacts. 
They store data as sets of key-value pairs or plain configuration files in any format.
ConfigMap can be used in several different ways. 
-A container can use the data as environmental variables from one or more sources. 
-The values contained inside can be passed to commands inside the pod.

config.js : you have a file on your local filesystem called
-You can create a ConfigMap that contains this file

$ kubectl get configmap foobar -o yaml

ConfigMaps can be consumed in various ways:
-Pod environmental variables from single or multiple ConfigMaps
-Use ConfigMap values in Pod commands
-Populate Volume from ConfigMap
-Add ConfigMap data to specific path in Volume
-Set file names and access mode in Volume from ConfigMap data
-Can be used by system components and controllers.

>>Using ConfigMaps
You can use ConfigMaps as environment variables or using a volume mount. 
your pod manifest will use the valueFrom key and the configMapKeyRef value to read the values.

env:
- name: SPECIAL_LEVEL_KEY
  valueFrom:
    configMapKeyRef:
      name: special-config
      key: special.how

With volumes, you define a volume with the configMap type in your pod and mount it where it needs to be used:

volumes:
- name: config-volume
  configMap:
    name: special-config


*** SERVICES ***
A Service is an operator running inside the kube-controller-manager, which sends API calls via the kube-apiserver to the Network Plugin (such as Calico) and the kube-proxy pods running all nodes. 
The Service operator also creates an Endpoint operator, which queries for the ephemeral IP addresses of pods with a particular label. 
These agents work together to manage firewall rules using iptables or ipvs.

>>Accessing an Application with a Service

$ kubectl expose deployment/nginx --port=80 --type=NodePort
$ kubectl get svc
$ kubectl get svc nginx -o yaml

Open browser http://Public-IP:31230.

The kubectl expose command created a service for the nginx deployment. 
This service used port 80 and generated a random port on all the nodes. 
A particular port and targetPort can also be passed during object creation to avoid random values. 
The targetPort defaults to the port, but could be set to any value, including a string referring to a port on a backend Pod.

The kubectl get svc command gave you a list of all the existing services, and we saw the nginx service, which was created with an internal cluster IP.

>>Service Types
>CLusterIP
ClusterIP service type is the default, and only provides access internally
>NodePOrt
NodePort type is great for debugging, or when a static IP address is necessary, such as opening a particular address through a firewall. 
>LoadBalancer
The LoadBalancer service was created to pass requests to a cloud provider like GKE or AWS. 
>ExternalName
It has no selectors, nor does it define ports or endpoints. It allows the return of an alias to an external service. 
The redirection happens at the DNS level, not via a proxy or forward. 

kubectl proxy command creates a local service to access a ClusterIP.

>>Local Proxy for Development
When developing an application or service, one quick way to check your service is to run a local proxy with kubectl. 
When running, you can make calls to the Kubernetes API on localhost and also reach the ClusterIP services on their API URL. 
$ kubectl proxy

>>DNS
DNS has been provided as CoreDNS by default as of v1.13. The use of CoreDNS allows for a great amount of flexibility. 
Once the container starts, it will run a Server for the zones it has been configured to serve.
$kube-dns


*** HELM ***
With Helm, you can package all those manifests and make them available as a single tarball. 
You can put the tarball in a repository, search that repository, discover an application, and then, with a single command, deploy and start the entire application, one or more times.

>>Charts
chart is an archived set of Kubernetes resource manifests that make up a distributed application.
── Chart.yaml			contains some metadata about the Chart, like its name, version, keywords,
├── README.md
├── templates			contains the resource manifests that make up this MariaDB application.
│   ├── NOTES.txt
│   ├── _helpers.tpl
│   ├── configmap.yaml
│   ├── deployment.yaml
│   ├── pvc.yaml
│   ├── secrets.yaml
│   └── svc.yaml
└── values.yaml			contains keys and values that are used to generate the release in your cluster. 

>>Templates
templates are resource manifests that use the Go templating syntax. 
Variables defined in the values file, for example, get injected in the template when a release is created.

>>Chart Repositories and Hub
Repositories are currently simple HTTP servers that contain an index file and a tarball of all the Charts present.
helm search hub command.

$ helm search hub redis
$ helm repo add bitnami ht‌tps://charts.bitnami.com/bitnami
$ helm repo list
$ helm search repo bitnami

>>Deploying a Chart
helm install  		For deployin chart

resources need to exist is by reading the READMEs for each chart.
$ helm fetch bitnami/apache --untar
$ cd apache/
$ ls

$ helm install anotherweb


*** INGRESS ***
Ingress Controllers and Rules to do the same function as Service( expose a containerized application outside of the cluster.)
You can deploy multiple controllers, each with unique configurations.

>>Ingress Controller
An Ingress Controller is a daemon running in a Pod which watches the /ingresses endpoint on the API server, which is found under the networking.k8s.io/v1beta1 group for new objects. 
When a new endpoint is created, the daemon uses the configured set of rules to allow inbound connection to a service, most often HTTP traffic. 
This allows easy access to a service through an edge router to Pods, regardless of where the Pod is deployed. 

>>nginx
nginx as a ingress controller
Customization can be done via a ConfigMap, Annotations, or, for detailed configuration, a custom template:
Uses the annotation kubernetes.io/ingress.class: "nginx"
L7 traffic requires the proxy-real-ip-cidr setting
Does not use conntrack entries for iptables DNAT

>>Ingress API Resources
rules:
  - host: ghost.192.168.99.100.nip.io
    http:
      paths:
      - backend:
          service
            name: ghost
            port:
              number: 2368 
        path: /
        pathType: ImplementationSpecific

$ kubectl get ingress
$ kubectl delete ingress <ingress_name>
$ kubectl edit ingress <ingress_name>

>>Deploying the Ingress Controller

$ kubectl create -f backend.yaml
$ kubectl get pods,rc,svc

>>Creating an Ingress Rule
start a ghost deployment and expose it with an internal ClusterIP service. 

$ kubectl run ghost --image=ghost
$ kubectl expose deployments ghost --port=2368

kind: Ingress
metadata:
rules:
  - host: ghost.192.168.99.100.nip.io
    http:
      paths:
      - backend:
          service
            name: ghost
            port:
              number: 2368

>>Intelligent Connected Proxies
service mesh consists of edge and embedded proxies communicating with each other and handling traffic based on rules from a control plane. 
Various options are available, including Envoy, Istio, and linkerd.

*** SCHEDULING ***
 The kube-scheduler determines which nodes will run a Pod, using a topology-aware algorithm. 
The default scheduling decision can be affected through the use of Labels on nodes or Pods. Labels of podAffinity, taints, and pod bindings allow for configuration from the Pod or the node perspective.
Pods from a node should the required condition no longer be true, such as requiredDuringScheduling, RequiredDuringExecution. 

>>Filtering (Predicates)
The scheduler goes through a set of filters, or predicates, to find available nodes, then ranks each node using priority functions.
The predicates, such as PodFitsHost or NoDiskConflict, are evaluated in a particular and configurable order.
HostNamePred, which is also known as HostName, which filters out nodes that do not match the node name specified in the pod specification. 
Another predicate is PodFitsResources to make sure that the available CPU and memory can fit the resources required by the Pod. 

>>Scoring (Priorities)
Priorities are functions used to weight resources. 
Unless Pod and node affinity has been configured to the SelectorSpreadPriority setting, which ranks nodes based on the number of existing running pods, they will select the node with the least amount of Pods. 

ImageLocalityPriorityMap favors nodes which already have downloaded container images. 
You can view a list of priorities at cp/pkg/scheduler/algorithm/priorities.

>>Scheduling Policies
The default scheduler contains a number of predicates and priorities; however, these can be changed via a scheduler policy file. 
you will configure a scheduler with this policy using the --policy-config-file parameter and 
define a name for this scheduler using the --scheduler-name parameter

>>Specifying the Node Label

nodeSelector field in a pod specification provides a straightforward way to target a node or a set of nodes, using one or more key-value pairs.

spec:
  containers:
  - name: redis
    image: redis
  nodeSelector:
    net: fast

Setting the nodeSelector tells the scheduler to place the pod on a node that matches the labels.

>>Scheduler Profiles
Another way to configure the scheduler is via the use of scheduling profiles
An extension point is one of the twelve stages of scheduling, at which point a plugin can be used to modify how that state of a scheduler works:-
-queueSort
-preFilter
-filter
-postFilter
-preScore
-score
-reserve
-permit
-preBind
-bind
-postBind
-multiPoint

>>podAffinity
The Pod can be scheduled on a node running a Pod with a key label of security and a value of S1. 
If this requirement is not met, the Pod will remain in a Pending state.

>>podAntiAffinity
With podAntiAffinity, we can prefer to avoid nodes with a particular label. 

>>Node Affinity Rules
Pod affinity/anti-affinity has to do with other Pods, the use of nodeAffinity allows Pod scheduling based on node labels. 
This is similar and will some day replace the use of the nodeSelector setting.

nodeAffinity prefers a node with the above rule, but the pod would be scheduled even if there were no matching nodes. 
The rule gives extra weight to nodes with a key of diskspeed with a value of fast or quick. 

>>Taints
A taint is expressed as key=value:effect. 
The key and the value are created by the administrator.

NoSchedule: The scheduler will not schedule a Pod
PreferNoSchedule: The scheduler will avoid using this node, unless there are no untainted nodes for the Pods toleration.
NoExecute: This taint will cause existing Pods to be evacuated and no future Pods scheduled.

If a node has multiple taints, the scheduler ignores those with matching tolerations. The remaining unignored taints have their typical effect. 

*** LOGGING AND TROUBLESHOOTING ***

$ kubectl create deploy busybox --image=busybox --command sleep 3600
$ kubectl exec -ti <busybox_pod> -- /bin/sh 

If the Pod is running, use kubectl logs pod-name to view the standard out of the container.
The next place to check is networking, including DNS, firewalls and general connectivity, using standard Linux commands and tools. 

Errors from the command line
Pod logs and state of Pods
Use shell to troubleshoot Pod DNS and network
Check node logs for errors, make sure there are enough resources allocated
RBAC, SELinux or AppArmor for security settings​
API calls to and from controllers to kube-apiserver
Enable auditing
Inter-node network issues, DNS and firewall
Control Plane server controllers

>>Krew
 krew (the kubectl plugin manager) allows for cross-platform packaging and a helpful plugin index, which makes finding new plugins easy.
$ kubectl krew help

install	​	 kubectl plugins
list		 installed kubectl plugins
search		Discover kubectl plugins
uninstall	​Uninstall plugins
update		Update the local copy of the plugin index
upgrade		Upgrade installed plugins to newer versions
version		Show krew version and diagnostics

>>Managing Plugins
After installation ensure the $PATH includes the plugins. 
krew should allow easy installation and use after that.

$ export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"
$ kubectl krew search
$ kubectl krew install tail

kubectl plugin list
kubectl krew search
kubectl krew install new-plugin

>>Sniffing Traffic With Wireshark
sniff plugin you can view the traffic from within. 

The sniff command will use the first found container unless you pass the -c option to declare which container in the pod to use for traffic monitoring.
$ kubectl krew install sniff nginx-123456-abcd -c webcont

>>Logging Tools
you can use for logging, the Elasticsearch, Logstash, and Kibana Stack (ELK) has become quite common.

In Kubernetes, the kubelet writes container logs to local files. The kubectl logs command allows you to retrieve these logs.
Cluster-wide, you can use Fluentd to aggregate logs. 


*** SECURITY ***

>>Accessing the API
To perform any action in a Kubernetes cluster, you need to access the API and go through three main steps:

Authentication (token):
Authorization (RBAC):
Admission Controllers.

>>Authentication

There are three main points to remember with authentication in Kubernetes:
-In its straightforward form, authentication is done with certificates, tokens or basic authentication (i.e. username and password).
-Users are not created by the API, but should be managed by an external system.
-System accounts are used by processes to access the API

The type of authentication used is defined in the kube-apiserver startup options
--basic-auth-file
--oidc-issuer-url
--token-auth-file
--authorization-webhook-config-file

>>Authentication
There are two main authorization modes and two global Deny/Allow settings. The main modes are:

RBAC
Webhook.

They can be configured as kube-apiserver startup options:
--authorization-mode=RBAC
--authorization-mode=Webhook
--authorization-mode=AlwaysDeny
--authorization-mode=AlwaysAllow

>>Pod Security Policies
To automate the enforcement of security contexts, you can define PodSecurityPolicies (PSP)
For instance, if you do not want any of the containers in your cluster to run as the root user, you can define a PSP to that effect.

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: MustRunAsNonRoot
  fsGroup:
    rule: RunAsAny

PodSecurityPolicy -> Pod Security Policies to be enabled, you need to configure the admission controller of the controller-manager

>>Network Security Policies
all pods can reach each other; all ingress and egress traffic is allowed. This has been a high-level networking requirement in Kubernetes
This is done by configuring a NetworkPolicy.

The spec of the policy can narrow down the effect to a particular namespace, which can be handy. Further settings include a podSelector, or label, to narrow down which Pods are affected.

>>Network Security Policy Example

Only Pods with the label of role: db will be affected by this policy, and the policy has both Ingress and Egress settings.
The ingress setting includes a 172.17 network, with a smaller range of 172.17.1.0 IPs being excluded from this traffic.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-egress-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - ipBlock:
            cidr: 172.17.0.0/16
            except:
              - 172.17.1.0/24
        - namespaceSelector:
            matchLabels:
              project: myproject
        - podSelector:
            matchLabels:
              role: frontend
      ports:
        - protocol: TCP
          port: 6379
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/24
      ports:
        - protocol: TCP
          port: 5978

Use another dedicated NetworkPolicy instead.
podSelector:
  matchExpressions:
    - {key: inns, operator: In, values: ["yes"]}

>>Default Policy Example
The empty braces will match all Pods not selected by other NetworkPolicy and will not allow ingress traffic. Egress traffic would be unaffected by this policy.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress

Some network plugins, such as WeaveNet, may require annotation of the Namespace. The following shows the setting of a DefaultDeny for the myns namespace:

kind: Namespace
apiVersion: v1
metadata:
  name: myns
  annotations:
    net.beta.kubernetes.io/network-policy: |
     {
        "ingress": {
          "isolation": "DefaultDeny"
        }
     }



























































































































































































































































































































































































































































































